{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_points_kernels as tp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_points3d.core.common_modules import FastBatchNorm1d\n",
    "from torch_points3d.modules.KPConv.kernels import KPConvLayer\n",
    "from torch_geometric.nn import voxel_grid\n",
    "\n",
    "\n",
    "from lib.pointops2.functions import pointops\n",
    "\n",
    "from util.data_util import collate_fn_limit\n",
    "from util.s3dis import S3DIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'cuda:7'\n",
    "cuda_idx = 7\n",
    "device = torch.device(dev)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 204 samples in train set.\n"
     ]
    }
   ],
   "source": [
    "data_root = '/home/data/stanford_indoor3d'\n",
    "train_transform = None\n",
    "train_data = S3DIS(split='train', data_root=data_root, test_area=5, voxel_size=0.04, voxel_max=80000,\n",
    "                   transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=False, \n",
    "                          drop_last=True, collate_fn=partial(collate_fn_limit, max_batch_points=200000, logger=None)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_sample(pos, batch, size, start, return_p2v=True):\n",
    "    # pos: float [N, 3]\n",
    "    # batch: long [N]\n",
    "    # size: float [3, ]\n",
    "    # start: float [3, ] / None\n",
    "\n",
    "    cluster = voxel_grid(pos, batch, size, start=start) #[N, ]\n",
    "\n",
    "    if return_p2v == False:\n",
    "        unique, cluster = torch.unique(cluster, sorted=True, return_inverse=True)\n",
    "        return cluster\n",
    "\n",
    "    unique, cluster, counts = torch.unique(cluster, sorted=True, return_inverse=True, return_counts=True)\n",
    "\n",
    "    # obtain p2v_map\n",
    "    n = unique.shape[0]\n",
    "    k = counts.max().item()\n",
    "    p2v_map = cluster.new_zeros(n, k) #[n, k]\n",
    "    mask = torch.arange(k).cuda().unsqueeze(0) < counts.unsqueeze(-1) #[n, k]\n",
    "    p2v_map[mask] = torch.argsort(cluster)\n",
    "\n",
    "    return cluster, p2v_map, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hog(xyz, window_size, offset, batch, window_maps=None, k=40, choice=\"knn\", radius=0.1):\n",
    "    '''\n",
    "    xyz: (n, 3) = coordinates of all points\n",
    "    window_size: float = size of window\n",
    "    offset: (batch_size) = batch offset\n",
    "    batch: (n, ) = index of the batch that the ith point belongs to\n",
    "    window_maps: (2, ) = v2p_map and p2v_map \n",
    "    k: int = number of nearest neighbors\n",
    "    choice: str = \"knn\" or \"ballquery\"\n",
    "    radius: float = radius of ball query\n",
    "    \n",
    "    Histogram of Oriented Gradients in Non-Overlapping Cubic Windows\n",
    "    \n",
    "    1. Query neighbors of all points\n",
    "    2. Reduce each neighborhood by applying SVD\n",
    "    3. Values obtained are s (magnitude) and v[0] (gradient)\n",
    "    4. get angles from gradients & convert angles to unsigned\n",
    "    5. initialize histogram for each cubic window as 2D (zenith & azimuth) array of 9 cells (20 degrees each)\n",
    "    6. each point contributes its gradient magntiude to the respective window of the histogram as per its angles\n",
    "    7. normalize gradients in window along dimension of bins\n",
    "    '''\n",
    "    # p2v_map: (num windows, max points per window) index of a point and the window that it has been mapped to\n",
    "    # v2p_map: (n) index of window that ith point has been mapped to\n",
    "    if window_maps is None:\n",
    "        v2p_map, p2v_map, counts = grid_sample(xyz, batch, window_size, start=None)\n",
    "    else:\n",
    "        v2p_map, p2v_map = window_maps\n",
    "    num_pts = xyz.size(0)\n",
    "    device = xyz.get_device()\n",
    "    # 1. Query neighbors of all points & get displacement to neighbors\n",
    "    if choice == \"knn\":\n",
    "        # neighbor_idx: (n, k) = indices of all points & their k nearest neighbors\n",
    "        neighbor_idx = pointops.knnquery(k, xyz, xyz, offset, offset)[\n",
    "            0].contiguous()\n",
    "    elif choice == \"ballquery\":\n",
    "        raise NotImplementedError(\"Ball Query not yet supported\")\n",
    "        # neighbor_idx = tp.ball_query(radius, k, xyz, xyz, mode=\"partial_dense\", batch_x=batch, batch_y=batch)[0].contiguous()\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Invalid option for choice of nearest neighbors\")\n",
    "    # (n, 3)\n",
    "    mean_per_nbrhood = torch.index_select(\n",
    "        xyz, dim=0, index=neighbor_idx.flatten()).view(-1, 40, 3).mean(dim=1)\n",
    "    # center the data\n",
    "    xyz = xyz - mean_per_nbrhood\n",
    "    # get displacement to neighbors\n",
    "    disp = pointops.subtraction(xyz, xyz, neighbor_idx.int())\n",
    "    # 0 distance to itself\n",
    "    if choice == \"ballquery\":\n",
    "        disp[neighbor_idx == -1] = 0\n",
    "    # 2. Reduce each neighborhood by applying SVD\n",
    "    _, s, v = np.linalg.svd(disp.cpu().numpy(), full_matrices=False)\n",
    "    # 3. Values obtained are s[0] (magnitude) and v[0] (gradient)\n",
    "    magnitudes = torch.from_numpy(s[:, 0]).cuda(device)  # N x 3 -> N\n",
    "    gradients = torch.from_numpy(v[:, :, 0]).cuda(device)  # N x 3 x 3 -> N x 3\n",
    "    # 4. get angles from gradients & convert angles to unsigned\n",
    "    zenith = torch.acos(gradients[:, 2]) * 180 / np.pi\n",
    "    zenith[zenith < 0] += 180\n",
    "    # add 1e-12 for safe division\n",
    "    azimuth = torch.atan(\n",
    "        torch.div(gradients[:, 1], gradients[:, 0] + 1e-12)) * 180 / np.pi\n",
    "    azimuth[azimuth < 0] += 180\n",
    "    # 5. initialize histogram\n",
    "    histogram = torch.zeros((num_pts, 18), device=f\"cuda:{device}\")\n",
    "    # histogram = torch.zeros((num_pts, 18))\n",
    "    # 6. fill histogram with gradients based on angle values\n",
    "    for i, angles in enumerate([zenith, azimuth]):\n",
    "        # which bin in its respective histogram\n",
    "        # does each point contribute to\n",
    "        bins = (torch.floor(angles / 20.0 - 0.5) % 9).int()\n",
    "        # vote for first bin\n",
    "        width = 20.0\n",
    "        num_bins = 9\n",
    "        first_centers = width * ((bins + 1) % num_bins + 0.5)\n",
    "        first_votes = magnitudes * ((first_centers - angles) % 180) / width\n",
    "        # vote for second bin\n",
    "        second_centers = width * (bins + 0.5)\n",
    "        second_votes = magnitudes * ((angles - second_centers) % 180) / width\n",
    "        first_bin_sums = torch.zeros(\n",
    "            (p2v_map.size(0), 9), device=f\"cuda:{device}\")\n",
    "        second_bin_sums = torch.zeros(\n",
    "            (p2v_map.size(0), 9), device=f\"cuda:{device}\")\n",
    "        for c in range(9):\n",
    "            # sum the total contribution of magnitudes to this bin\n",
    "            first_bin_contributors = first_votes * (bins == c)\n",
    "            second_bin_contributors = second_votes * (bins == c)\n",
    "            # split into sum per window (num_windows, )\n",
    "            first_bin_sum = torch.index_select(first_bin_contributors,\n",
    "                                               dim=0,\n",
    "                                               index=p2v_map.flatten()).reshape(p2v_map.shape).sum(dim=1)\n",
    "            second_bin_sum = torch.index_select(second_bin_contributors,\n",
    "                                                dim=0,\n",
    "                                                index=p2v_map.flatten()).reshape(p2v_map.shape).sum(dim=1)\n",
    "            # remove redundant contribution of index 0\n",
    "            num_zeros_per_window = (p2v_map == 0).sum(dim=1)\n",
    "            if first_bin_contributors[0] > 0:\n",
    "                first_bin_sum = first_bin_sum - \\\n",
    "                    first_bin_contributors[0] * num_zeros_per_window\n",
    "                # add back index 0's actual contribution\n",
    "                first_bin_sum[v2p_map[0]] += first_bin_contributors[0]\n",
    "            if second_bin_contributors[0] > 0:\n",
    "                second_bin_sum = second_bin_sum - \\\n",
    "                    second_bin_contributors[0] * num_zeros_per_window\n",
    "                second_bin_sum[v2p_map[0]] += second_bin_contributors[0]\n",
    "            first_bin_sums[:, c] = first_bin_sum\n",
    "            second_bin_sums[:, c] = second_bin_sum\n",
    "        # 7. normalize sum of gradients over dimension of bins\n",
    "        first_bin_sums = F.normalize(first_bin_sums, p=2.0, dim=1)\n",
    "        second_bin_sums = F.normalize(second_bin_sums, p=2.0, dim=1)\n",
    "        # add to histogram\n",
    "        for c in range(9):\n",
    "            if i == 0:\n",
    "                '''\n",
    "                v2p_map maps each window to a point, \n",
    "                    so its shape is (number_of_points, ) \n",
    "                    and each element x is 0 <= x < num_windows\n",
    "                first_bin_sums contains the histogram \n",
    "                    (both zenith & azimuth) of each window\n",
    "                the below operation maps the histogram to each pixel\n",
    "                    depending on which window it belongs to  \n",
    "                '''\n",
    "                histogram[:, c] += torch.gather(first_bin_sums[:, c],\n",
    "                                                dim=0, index=v2p_map)\n",
    "                histogram[:, (c + 1) % 9] += torch.gather(\n",
    "                    second_bin_sums[:, c], dim=0, index=v2p_map)\n",
    "            else:\n",
    "                histogram[:, c + 9] += torch.gather(first_bin_sums[:, c], \n",
    "                                                    dim=0, index=v2p_map)\n",
    "                histogram[:, (c + 1) % 9 + 9] += torch.gather(\n",
    "                    second_bin_sums[:, c], dim=0, index=v2p_map)\n",
    "    return histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset= tensor([35883, 71634], dtype=torch.int32)\n",
      "offset_= tensor([35883, 35751], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "max_num_neighbors = 40\n",
    "# (n, 3), (n, c), (n), (b)\n",
    "coord, feat, target, offset = iter(train_loader).next()\n",
    "offset_ = offset.clone()\n",
    "offset_[1:] = offset_[1:] - offset_[:-1]\n",
    "print('offset=', offset)\n",
    "print('offset_=', offset_)\n",
    "batch = torch.cat([torch.tensor([ii]*o) for ii, o in enumerate(offset_)], 0).long()\n",
    "\n",
    "sigma = 1.0\n",
    "radius = 2.5 * 0.04 * sigma\n",
    "neighbor_idx = tp.ball_query(radius, max_num_neighbors, coord, coord, mode=\"partial_dense\", batch_x=batch, batch_y=batch)[0]\n",
    "\n",
    "coord, feat, target, offset, batch, neighbor_idx = coord.cuda(non_blocking=True), feat.cuda(non_blocking=True), \\\n",
    "                                    target.cuda(non_blocking=True), offset.cuda(non_blocking=True), \\\n",
    "                                    batch.cuda(non_blocking=True), neighbor_idx.cuda(non_blocking=True)\n",
    "assert batch.shape[0] == feat.shape[0]\n",
    "feat = torch.cat([feat, coord], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuck_gpu():\n",
    "    torch.index_select(coord, dim=0, index=neighbor_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuck_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size 0.04\n",
      "window_size [0.16, 0.32, 0.64, 1.28]\n",
      "grid_sizes [0.04, 0.08, 0.16, 0.32]\n",
      "quant_sizes [0.01, 0.02, 0.04, 0.08]\n"
     ]
    }
   ],
   "source": [
    "quant_size = 0.01\n",
    "patch_size = 1\n",
    "grid_size = 0.04\n",
    "window_size = 4\n",
    "num_layers = 4\n",
    "\n",
    "patch_size = grid_size * patch_size\n",
    "window_sizes = [patch_size * window_size * (2**i) for i in range(num_layers)]\n",
    "grid_sizes = [patch_size * (2**i) for i in range(num_layers)]\n",
    "quant_sizes = [quant_size * (2**i) for i in range(num_layers)]\n",
    "\n",
    "print(\"patch_size\", patch_size)\n",
    "print(\"window_size\", window_sizes)\n",
    "print(\"grid_sizes\", grid_sizes)\n",
    "print(\"quant_sizes\", quant_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram = compute_hog(xyz=coord, window_size=window_sizes[0], offset=offset, batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disp torch.Size([71634, 40, 3])\n"
     ]
    }
   ],
   "source": [
    "window_size = window_sizes[0]\n",
    "histogram = compute_hog(coord, window_size, offset, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pt10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0813755b8afe6c57626065f8754f9d5391594048ab82d14c26059a271a79aa57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
